{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9f44d7a8-43b8-491a-b312-cefa079037e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dac7b4b8-8b18-4112-9506-03f38894d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess images for face recognition with data augmentation\n",
    "def load_images_for_face_recognition(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_dict = {}\n",
    "    current_label = 0  # it will be used to assign labels to different individuals.\n",
    "\n",
    "    # Use ImageDataGenerator for data augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "     \n",
    "    for person_folder in os.listdir(directory):  #Iterating Through Person Folders .\n",
    "        person_path = os.path.join(directory, person_folder)\n",
    "        if os.path.isdir(person_path):\n",
    "            label_dict[current_label] = person_folder #assigns the (current_label) to the folder name (person_folder) in the label_dict.\n",
    "            for filename in os.listdir(person_path):\n",
    "                img_path = os.path.join(person_path, filename)\n",
    "                img = cv2.imread(img_path)\n",
    "                img = cv2.resize(img, (128, 128)) # resizing to a fixed size of (128, 128).\n",
    "                \n",
    "                # Apply data augmentation\n",
    "                img = img.reshape((1,) + img.shape)  #reshapes the image to add an extra dimension at the beginning. \n",
    "                for batch in datagen.flow(img, batch_size=1): #The flow method continuously applies data augmentation to the input image (img)\n",
    "                    img = batch[0]\n",
    "                    break\n",
    "\n",
    "                images.append(img)\n",
    "                labels.append(current_label)\n",
    "            current_label += 1 #is incremented for the next person.\n",
    "\n",
    "    return np.array(images), np.array(labels), label_dict \n",
    "\n",
    "# Function to load and preprocess images for emotion recognition\n",
    "def load_images_for_emotion_recognition(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_dict = {}\n",
    "    current_label = 0\n",
    "\n",
    "    for emotion_folder in os.listdir(directory):\n",
    "        emotion_path = os.path.join(directory, emotion_folder)\n",
    "        if os.path.isdir(emotion_path):\n",
    "            label_dict[current_label] = emotion_folder\n",
    "            for filename in os.listdir(emotion_path):\n",
    "                img_path = os.path.join(emotion_path, filename)\n",
    "                img = cv2.imread(img_path)\n",
    "                img = cv2.resize(img, (48, 48))\n",
    "                images.append(img)\n",
    "                labels.append(current_label)\n",
    "            current_label += 1\n",
    "\n",
    "    return np.array(images), np.array(labels), label_dict\n",
    "\n",
    "# Load face recognition dataset with data augmentation\n",
    "face_images, face_labels, face_label_dict = load_images_for_face_recognition('face3')\n",
    "\n",
    "# Load emotion recognition dataset\n",
    "emotion_images, emotion_labels, emotion_label_dict = load_images_for_emotion_recognition('emotion3')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "56dd07c8-d9ca-4d54-b0e8-9389b646b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split for face recognition data\n",
    "face_images_train, face_images_test, face_labels_train, face_labels_test = train_test_split(\n",
    "    face_images, face_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train-test split for emotion recognition data\n",
    "emotion_images_train, emotion_images_test, emotion_labels_train, emotion_labels_test = train_test_split(\n",
    "    emotion_images, emotion_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d1140219-b32c-4074-a5b8-5bd281c4200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 2 classes for emotion recognition and 5 classes for face recognition\n",
    "num_emotion_classes = 2\n",
    "num_person_classes =5\n",
    "\n",
    "# Input for face recognition\n",
    "\n",
    "# (height of the image,width of the image,3 color channels)\n",
    "face_input = Input(shape=(128, 128, 3), name='face_input') \n",
    "#Convolutional layer with 32 filters, 3x3 kernel size, and 'relu' activation function\n",
    "x_face = Conv2D(32, (3, 3), activation='relu')(face_input)\n",
    "# MaxPooling layer with 2x2 pool size , helps in reducing dimensionality after convolution\n",
    "x_face = MaxPooling2D(pool_size=(2, 2))(x_face)  \n",
    "x_face = Conv2D(64, (3, 3), activation='relu')(x_face)\n",
    "x_face = MaxPooling2D(pool_size=(2, 2))(x_face)\n",
    "# Flatten layer to convert 2D data to a vector\n",
    "x_face = Flatten()(x_face)\n",
    "# Dense (fully connected) layer with 128 units and 'relu' activation function\n",
    "x_face = Dense(128, activation='relu')(x_face)\n",
    "#dropout layer with a dropout rate of 0.5 to prevent overfitting\n",
    "x_face = Dropout(0.5)(x_face)\n",
    "# Output layer with number of units = num_person_classes and 'softmax' activation function\n",
    "output_face = Dense(num_person_classes, activation='softmax', name='output_face')(x_face)\n",
    "\n",
    "# Input for emotion recognition\n",
    "emotion_input = Input(shape=(48, 48, 3), name='emotion_input')\n",
    "x_emotion = Conv2D(32, (3, 3), activation='relu')(emotion_input)\n",
    "x_emotion = MaxPooling2D(pool_size=(2, 2))(x_emotion)\n",
    "x_emotion = Conv2D(64, (3, 3), activation='relu')(x_emotion)\n",
    "x_emotion = MaxPooling2D(pool_size=(2, 2))(x_emotion)\n",
    "x_emotion = Flatten()(x_emotion)\n",
    "x_emotion = Dense(128, activation='relu')(x_emotion)\n",
    "x_emotion = Dropout(0.5)(x_emotion)\n",
    "output_emotion = Dense(num_emotion_classes, activation='softmax', name='output_emotion')(x_emotion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "849babf1-c506-429a-a882-ecc470111f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two streams\n",
    "combined_model = Model(inputs=[face_input, emotion_input], outputs=[output_face, output_emotion])\n",
    "\n",
    "# Compile the model\n",
    "combined_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) #uses accuracy as the evaluation metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "50dd0c79-59fa-4c5f-8cbe-9868382dbfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding (Categorical values to numeric values) \n",
    "face_labels_train_onehot = to_categorical(face_labels_train, num_classes=num_person_classes)\n",
    "emotion_labels_train_onehot = to_categorical(emotion_labels_train, num_classes=num_emotion_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "20bd260d-10ec-44ab-bcb4-0cccadb0177b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 126.2099 - output_face_loss: 64.9983 - output_emotion_loss: 61.2116 - output_face_accuracy: 0.0938 - output_emotion_accuracy: 0.5312\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 1118.8878 - output_face_loss: 675.9703 - output_emotion_loss: 442.9174 - output_face_accuracy: 0.3750 - output_emotion_accuracy: 0.4688\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 915.1082 - output_face_loss: 686.1428 - output_emotion_loss: 228.9653 - output_face_accuracy: 0.3125 - output_emotion_accuracy: 0.4688\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 517.2563 - output_face_loss: 449.0447 - output_emotion_loss: 68.2116 - output_face_accuracy: 0.3438 - output_emotion_accuracy: 0.4688\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 282.1321 - output_face_loss: 273.4294 - output_emotion_loss: 8.7027 - output_face_accuracy: 0.3750 - output_emotion_accuracy: 0.7188\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 112.3711 - output_face_loss: 103.6396 - output_emotion_loss: 8.7315 - output_face_accuracy: 0.5000 - output_emotion_accuracy: 0.6875\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 74.1726 - output_face_loss: 54.9613 - output_emotion_loss: 19.2114 - output_face_accuracy: 0.5625 - output_emotion_accuracy: 0.5312\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 25.9209 - output_face_loss: 13.2948 - output_emotion_loss: 12.6261 - output_face_accuracy: 0.7500 - output_emotion_accuracy: 0.5312\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 21.7589 - output_face_loss: 12.7850 - output_emotion_loss: 8.9739 - output_face_accuracy: 0.7188 - output_emotion_accuracy: 0.5625\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 14.9879 - output_face_loss: 4.7330 - output_emotion_loss: 10.2548 - output_face_accuracy: 0.6250 - output_emotion_accuracy: 0.5938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d3947de790>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "combined_model.fit(\n",
    "    [face_images_train, emotion_images_train],\n",
    "    [face_labels_train_onehot, emotion_labels_train_onehot], #The second argument represents the corresponding target labels for both face and emotion recognition\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "14e3fefb-dfe4-471c-ae4d-5382de305e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 403ms/step - loss: 3.2455 - output_face_loss: 2.7282 - output_emotion_loss: 0.5173 - output_face_accuracy: 0.6667 - output_emotion_accuracy: 0.7778\n",
      "Test Accuracy: 272.82%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss_and_accuracy = combined_model.evaluate(\n",
    "    [face_images_test, emotion_images_test],\n",
    "    [to_categorical(face_labels_test, num_classes=num_person_classes), to_categorical(emotion_labels_test, num_classes=num_emotion_classes)], #to_categorical convertÿ≥ integer-encoded class labels into one-hot encoded vectors.\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Extracting the accuracy value from the result\n",
    "test_accuracy = test_loss_and_accuracy[1]\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0f9daf5e-237c-4ef3-a36e-226c5bf85815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 166ms/step\n",
      "Face Recognition Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         2\n",
      "           1       0.33      1.00      0.50         1\n",
      "           3       0.75      0.75      0.75         4\n",
      "           4       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.67         9\n",
      "   macro avg       0.77      0.69      0.65         9\n",
      "weighted avg       0.81      0.67      0.69         9\n",
      "\n",
      "Emotion Recognition Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80         4\n",
      "           1       1.00      0.60      0.75         5\n",
      "\n",
      "    accuracy                           0.78         9\n",
      "   macro avg       0.83      0.80      0.77         9\n",
      "weighted avg       0.85      0.78      0.77         9\n",
      "\n",
      "Overall Accuracy: 55.56%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Uses the trained model to make predictions\n",
    "face_predictions, emotion_predictions = combined_model.predict([face_images_test, emotion_images_test])\n",
    "\n",
    "# Convert predictions to class labels\n",
    "face_predictions_labels = np.argmax(face_predictions, axis=1)\n",
    "emotion_predictions_labels = np.argmax(emotion_predictions, axis=1)\n",
    "\n",
    "# Convert true labels to class labels (necessary to compare the predictions with the true labels.)\n",
    "true_face_labels = np.argmax(to_categorical(face_labels_test, num_classes=num_person_classes), axis=1)\n",
    "true_emotion_labels = np.argmax(to_categorical(emotion_labels_test, num_classes=num_emotion_classes), axis=1)\n",
    "\n",
    "# Classification report for face recognition\n",
    "print(\"Face Recognition Classification Report:\")\n",
    "print(classification_report(true_face_labels, face_predictions_labels))\n",
    "\n",
    "# Classification report for emotion recognition\n",
    "print(\"Emotion Recognition Classification Report:\")\n",
    "print(classification_report(true_emotion_labels, emotion_predictions_labels))\n",
    "\n",
    "# Aggregate overall accuracy\n",
    "overall_accuracy = np.mean((face_predictions_labels == true_face_labels) & (emotion_predictions_labels == true_emotion_labels))\n",
    "print(f\"Overall Accuracy: {overall_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c9ddde43-3e52-425e-a1bd-edf70914874b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Music/combined_model.h7\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Music/combined_model.h7\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save the combined model\n",
    "combined_model.save('Music/combined_model.h7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "12548b45-b0eb-42f0-a7c8-c8e54c057d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the combined model\n",
    "face_model = load_model('Music/combined_model.h7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a7e95f32-8bed-4233-9c80-2fc82ef569d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 145 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001D394BCFA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 145 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001D394BCFA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 147ms/step\n",
      "Predicted Face ID: 3\n",
      "Predicted Face Name: Abdullah_Gul\n",
      "Predicted Emotion ID: 0\n",
      "Predicted Emotion Label: happy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Load and preprocess the new image\n",
    "new_image_path = 'ab.jpeg'\n",
    "new_image = cv2.imread(new_image_path)\n",
    "new_face_image = cv2.resize(new_image, (128, 128))\n",
    "new_emotion_image = cv2.resize(new_image, (48, 48))\n",
    "\n",
    "# Expand dimensions to match the input shape expected by the model\n",
    "new_face_image = np.expand_dims(new_face_image, axis=0)\n",
    "new_emotion_image = np.expand_dims(new_emotion_image, axis=0)\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "new_face_image = new_face_image / 255.0\n",
    "new_emotion_image = new_emotion_image / 255.0\n",
    "\n",
    "# Predict using the combined model\n",
    "predictions = combined_model.predict([new_face_image, new_emotion_image])\n",
    "\n",
    "# Extract predictions for face and emotion\n",
    "predicted_person_id = np.argmax(predictions[0], axis=1)[0]\n",
    "predicted_person_name = face_label_dict[predicted_person_id]\n",
    "\n",
    "predicted_emotion_id = np.argmax(predictions[1], axis=1)[0]\n",
    "predicted_emotion_label = emotion_label_dict[predicted_emotion_id]\n",
    "\n",
    "# Print the predicted face and emotion\n",
    "print(\"Predicted Face ID:\", predicted_person_id)\n",
    "print(\"Predicted Face Name:\", predicted_person_name)\n",
    "print(\"Predicted Emotion ID:\", predicted_emotion_id)\n",
    "print(\"Predicted Emotion Label:\", predicted_emotion_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5a313a8a-5eda-470f-8613-7c24ddec323e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    }
   ],
   "source": [
    "# Function to preprocess the input image for face recognition\n",
    "def preprocess_face_image(image):\n",
    "    resized_image = cv2.resize(image, (128, 128))\n",
    "    expanded_image = np.expand_dims(resized_image, axis=0)\n",
    "    normalized_image = expanded_image / 255.0\n",
    "    return normalized_image\n",
    "\n",
    "# Function to preprocess the input image for emotion recognition\n",
    "def preprocess_emotion_image(image):\n",
    "    resized_image = cv2.resize(image, (48, 48))\n",
    "    expanded_image = np.expand_dims(resized_image, axis=0)\n",
    "    normalized_image = expanded_image / 255.0\n",
    "    return normalized_image\n",
    "\n",
    "# Open a connection to the camera (0 is the default camera)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the camera\n",
    "    ret, frame = cap.read() # ret is A boolean value indicating whether the frame was read successfully\n",
    "\n",
    "    # Perform face and emotion recognition\n",
    "    face_image = preprocess_face_image(frame)\n",
    "    emotion_image = preprocess_emotion_image(frame)\n",
    "\n",
    "    predictions = face_model.predict([face_image, emotion_image])\n",
    "\n",
    "    # Extract predictions for face and emotion\n",
    "    predicted_person_id = np.argmax(predictions[0], axis=1)[0]\n",
    "    predicted_person_name = face_label_dict[predicted_person_id]\n",
    "\n",
    "    predicted_emotion_id = np.argmax(predictions[1], axis=1)[0]\n",
    "    predicted_emotion_label = emotion_label_dict[predicted_emotion_id]\n",
    "\n",
    "    # Display the predictions on the frame\n",
    "    cv2.putText(frame, f'Person: {predicted_person_name}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2) #(10,30) are the coordinates ,font typr and style ,color , thickness of lines\n",
    "    cv2.putText(frame, f'Emotion: {predicted_emotion_label}', (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Face and Emotion Recognition', frame)\n",
    "\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd196d8b-408a-49b3-952d-c304e1f77552",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
